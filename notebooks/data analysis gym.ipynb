{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nicholasjunge/Studies/Master Mathematics Munich/Masterarbeit/codebase/notebooks', '/Users/nicholasjunge/anaconda3/lib/python37.zip', '/Users/nicholasjunge/anaconda3/lib/python3.7', '/Users/nicholasjunge/anaconda3/lib/python3.7/lib-dynload', '', '/Users/nicholasjunge/.local/lib/python3.7/site-packages', '/Users/nicholasjunge/anaconda3/lib/python3.7/site-packages', '/Users/nicholasjunge/anaconda3/lib/python3.7/site-packages/aeosa', '/Users/nicholasjunge/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/Users/nicholasjunge/.ipython']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandarallel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-758b852b1f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandarallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandarallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandarallel'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time \n",
    "import sys; \n",
    "print(sys.path)\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Add directory above current directory to path\n",
    "\n",
    "if not \"..\" in sys.path:\n",
    "    sys.path.insert(0, '..')\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Sanitizing the user base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing a list of datatypes for the columns of customer base\n",
    "customer_dtypes = {'ISACTIVE': str, \"CUST_CODE\": str, 'CLASS_CODE': str, 'BUSINESS_CODE': str, 'SEX': str, 'NO_MAIL': str, \n",
    "                   'NO_SMS': str, 'COMPANY_CUST_CODE': str, 'IS_DROP_IN_CUSTOMER': str}\n",
    "\n",
    "#list of columns that will be parsed as dates\n",
    "customer_date_cols = ['EXPIRE_DATE', 'LAST_VISIT', 'DATE_SAVED', 'DATE_LAST_MODIFIED', 'RENEW_DATE', 'PAUSE_START', 'PAUSE_END', 'MEMBER_SINCE']\n",
    "\n",
    "#first row in RES_CARD.xlsx is a nonsense entry, so skip it. HEADER COUNTS --> FIRST ROW HAS INDEX 1\n",
    "rows_to_skip = [1]\n",
    "\n",
    "customer_base = pd.read_excel('../DATA_GYM/RES_CARD.xlsx', sheet_name='Sheet1', skiprows=rows_to_skip, dtype=customer_dtypes, parse_dates=customer_date_cols, index_col=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservation card imported above is a description of the customer base of all the gyms. Some of the most important columns are:\n",
    "\n",
    "CUST_CODE (string, format: int1DOTint2): Unique customer identifier. Perhaps translate to int for an easier handling? !! Some entries are screwed up, having \"~del_cust\" (deleted customer?) as CUST_CODE -> remove those from analysis\n",
    "\n",
    "EXPIRE_DATE (datetime object, format YYYY-MM-DD): Date of contract expiry.\n",
    "\n",
    "MEMBER_SINCE (datetime object, format YYYY-MM-DD): Date of contract signing. Sometimes day, sometimes second accuracy -> maybe truncate entries to day format.\n",
    "\n",
    "--> The membership duration for a given customer cust_id is then just reservation_card[cust_id][EXPIRE_DATE] - reservation_card[cust_id][MEMBER_SINCE].\n",
    "\n",
    "IS_ACTIVE (bool, encoded as F or T): Whether the contract is still active. For these, the expiry date is set to December 31st, 2100.\n",
    "\n",
    "SEX (string): Gender of customer. Either \"M\" (male) or \"F\" (female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_bool_cols = ['ISACTIVE', 'NO_SMS', 'NO_MAIL', 'IS_DROP_IN_CUSTOMER']\n",
    "\n",
    "#dict object mapping the boolean columns \n",
    "bool_converter = {'T': True, 'F': False}\n",
    "\n",
    "#convert boolean columns to bools \n",
    "for col in customer_bool_cols:\n",
    "    customer_base[col] = customer_base[col].map(bool_converter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"EXPIRE_DATE\" and \"PAUSE_START\" contain strings that let the datetime parsing fail. Therefore, we run it a second time with coercion (that is, putting NaT (not a timestamp) whenever an unparseable string occurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols_with_errs = [\"EXPIRE_DATE\", \"PAUSE_START\"]\n",
    "\n",
    "for col in date_cols_with_errs:\n",
    "    #makes the parser convert wrong timestamps into NaT (not a timestamp)\n",
    "    customer_base[col] = pd.to_datetime(customer_base[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many customers do not have contract start / end dates -> for these, we cannot do frequency analysis\n",
    "no_expiry = customer_base[customer_base[\"EXPIRE_DATE\"].isna()]\n",
    "no_start = customer_base[customer_base[\"MEMBER_SINCE\"].isna()]\n",
    "\n",
    "print(len(no_expiry.index))\n",
    "print(len(no_start.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up having 22 entries without contract expiry information. Since we cannot do anything with them without diving deeper into the excel sheet, we drop them from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no inplace=True argument because we create another dataframe\n",
    "sanitized_customer_base = customer_base.dropna(subset=[\"EXPIRE_DATE\"])\n",
    "print(sanitized_customer_base[sanitized_customer_base[\"EXPIRE_DATE\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_customer_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the contract duration in days by simply using the formula CONTRACT_DURATION = EXPIRE_DATE $-$ MEMBER_SINCE, using the column names in our DataFrame. We add this as a new column called \"CONTRACT_DURATION\".\n",
    "Unfortunately, some users have the placeholder date \"1899-12-30\" for the MEMBER_SINCE entry, which means we will not get a sensible contract duration period from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop users with nonsense \"MEMBER_SINCE\" entries\n",
    "sanitized_customer_base = sanitized_customer_base[sanitized_customer_base[\"MEMBER_SINCE\"] != \"1899-12-30\"]\n",
    "\n",
    "#sanitized_customer_base.MEMBER_SINCE.describe()\n",
    "\n",
    "inactive_users = sanitized_customer_base[sanitized_customer_base.ISACTIVE == False]\n",
    "\n",
    "weird_inactive_users = inactive_users[sanitized_customer_base.EXPIRE_DATE > \"2020-01-01\"]\n",
    "\n",
    "sanitized_customer_base.drop(weird_inactive_users.index, inplace=True)\n",
    "#drop users with nonsense \"EXPIRE_DATE\" entries\n",
    "\n",
    "sanitized_customer_base = sanitized_customer_base[sanitized_customer_base[\"EXPIRE_DATE\"] > \"2000-01-01\"] \n",
    "\n",
    "sanitized_customer_base.MEMBER_SINCE.describe()\n",
    "#sanitized_customer_base = sanitized_customer_base[inactive_users[\"EXPIRE_DATE\"] < \"2030-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_cust = sanitized_customer_base.iloc[0]\n",
    "\n",
    "print(ex_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = sanitized_customer_base.DATE_SAVED.max()\n",
    "sanitized_customer_base[\"CONTRACT_DURATION\"] = sanitized_customer_base.apply(lambda x: x.EXPIRE_DATE - x.MEMBER_SINCE if not x.ISACTIVE else last_date - x.MEMBER_SINCE, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_customer_base.EXPIRE_DATE.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the subsequent Time Series Analysis it is useful to know the time window that a user has been active. We can simply calculate this by the formula ACTIVE_DURATION = LAST_VISIT - MEMBER_SINCE if LAST_VISIT is not NaN, and ACTIVE_DURATION == 0 days if LAST_VISIT is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_customer_base[\"ACTIVE_DURATION\"] = np.where(sanitized_customer_base.LAST_VISIT.notnull(), sanitized_customer_base.LAST_VISIT - sanitized_customer_base.MEMBER_SINCE, pd.Timedelta(\"0 days\"))\n",
    "sanitized_customer_base[\"ACTIVE_DURATION\"] = pd.to_timedelta(sanitized_customer_base[\"ACTIVE_DURATION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some customers have no last visit entry despite having logins present in the data (see below). For this, we first check how many customers there are without a valid last visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {0} customers of {1} total customers without a last visit entry.\".format(len(sanitized_customer_base[sanitized_customer_base[\"LAST_VISIT\"].isna()].index),len(sanitized_customer_base.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_last_visit = sanitized_customer_base[sanitized_customer_base[\"LAST_VISIT\"].isnull()]\n",
    "users_no_last_visit = no_last_visit.index\n",
    "no_last_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_duration = sanitized_customer_base[sanitized_customer_base.CONTRACT_DURATION < pd.Timedelta(days=30)]\n",
    "sanitized_customer_base.drop(small_duration.index, inplace=True)\n",
    "sanitized_customer_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing a list of datatypes for the columns of customer base\n",
    "reservation_dtypes = {'ID': int, 'START_TIME': str, 'END_TIME': str, 'OBJ_DET_ID': int, 'DATE_SAVED': str, 'DATE_LAST_MODIFIED': str, \n",
    "            'INSTRUCTOR_CODE': str, 'SPACE_CODE': str, 'MAX_PEOPLE': int, 'OBJ_CODE': str, 'DESCRIPTION': str, 'LOCATION': int, \n",
    "                   'TYPE OF THE CLASS': int, 'Internal/extern': int}\n",
    "\n",
    "#list of columns that will be parsed as dates\n",
    "reservation_date_cols = ['START_TIME', 'END_TIME', 'DATE_SAVED', 'DATE_LAST_MODIFIED']\n",
    "\n",
    "reservation_table = pd.read_excel('../DATA_GYM/RESERVATION.xlsx', parse_dates=reservation_date_cols, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spreadsheet gives us the overview over the different classes that customers can make a reservation for. They are individual instances of gym events (things like spin classes, Spartan circle etc.). Important columns are:\n",
    "\n",
    "ID (integer): A simple integer identifier for the timeslot of the event.\n",
    "\n",
    "OBJ_DET_ID (integer): A complex integer identifier for the reservation encoding the timeslot and type of event at a location (e.g. yoga, spinning). Independent of INSTRUCTOR_CODE and OBJ_CODE, definitely dependent on START_TIME, END_TIME, possibly dependent on SPACE_CODE, MAX_PEOPLE. \n",
    "\n",
    "INSTRUCTOR_CODE (string): A string identifying the instructor.\n",
    "\n",
    "SPACE_CODE (string): A string identifying the specific location space of the event, in terms of LOCATION (see below).\n",
    "\n",
    "MAX_PEOPLE (integer): The maximum number of participants in the event.\n",
    "\n",
    "OBJ_CODE (string): A string identifying the login destination. This can be the gym (for people training outside of classes), a course, class ID etc.\n",
    "\n",
    "START_TIME, END_TIME (datetime objects, format YYYY-MM-DD HH:MM:SS): Start and end dates of the events.\n",
    "\n",
    "LOCATION (integer): Integer describing the location of the event. Is a weaker identifier than SPACE_CODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservation_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the RESERVATION_DET_xxxxxxx-xxxxxxx spreadsheets and write them into a big-ass dataframe \n",
    "file_prefix = \"../DATA_GYM/RESERVATION_DET_\"\n",
    "file_type = \".xlsx\"\n",
    "record_numbers = [str(i * int(1e6)) for i in range(1,11)]\n",
    "\n",
    "record_list = []\n",
    "\n",
    "#passing a list of datatypes for the columns\n",
    "login_dtypes = {\"ID\": int, \"RES_ID\": int, \"DATE_SAVED\": str, \"DATE_LAST_MODIFIED\": str, \"CUST_CODE\": str, \n",
    "            \"IS_ARRIVED\": str, \"LOCATION\": int, \"COMPUTED_RSV_ID\": str}\n",
    "\n",
    "#columns that are given as dates can be parsed while reading \n",
    "login_date_columns = ['DATE_SAVED', 'DATE_LAST_MODIFIED']\n",
    "\n",
    "#additional values that are recognized as NaN (use-case: ~del_cust as CUST_CODE)\n",
    "additional_nans = ['~del_cust']\n",
    "\n",
    "sheet_name = \"../DATA_GYM/RESERVATION_DET_1000000.xlsx\"\n",
    "\n",
    "single_sheet_start = time.time()\n",
    "df = pd.read_excel(sheet_name, index_col=0, dtype=login_dtypes, parse_dates=login_date_columns)\n",
    "single_sheet_end = time.time()\n",
    "record_list.append(df)\n",
    "print(\"The time for loading the first excel sheet only is {}\".format(single_sheet_end - single_sheet_start))\n",
    "\n",
    "nine_sheets_start = time.time()\n",
    "for i in range(len(record_numbers) - 1):\n",
    "    sheet_name = file_prefix + record_numbers[i] + \"-\" + record_numbers[i+1] + file_type\n",
    "    df = pd.read_excel(sheet_name, index_col=0, dtype=login_dtypes, parse_dates=login_date_columns)\n",
    "    record_list.append(df)\n",
    "nine_sheets_end = time.time()\n",
    "    \n",
    "login_data = pd.concat(record_list)\n",
    "print(\"The time for loading the other 9 sheets is {}\".format(nine_sheets_end - nine_sheets_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RESERVATION_DET_xxxxxxx-xxxxxxx spreadsheets give us the actual time series data. It contains the individual logins of the customers as well as the type of event they attended at which location. Important columns are:\n",
    "\n",
    "ID (integer): Login ID. Serves as index column in our login_data DataFrame.\n",
    "\n",
    "RES_ID (integer): Reservation object ID, corresponds to OBJ_DET_ID in the RESERVATION spreadsheet (see above).\n",
    "\n",
    "DATE_SAVED (datetime object, format DD.MM.YYYY HH:MM): Timestamp of the login that was saved.\n",
    "\n",
    "CUST_CODE (string, format int1DOTint2): Customer / member ID. Corresponds to the CUST_CODE column of the customer base spreadsheet RES_CARD. \n",
    "\n",
    "LOCATION (integer): Location identifier, should correspond to the column of the same name in RESERVATIONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipapo = login_data[login_data[\"CUST_CODE\"].isin(users_no_last_visit)]\n",
    "\n",
    "pipapo[\"CUST_CODE\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the workflow is clear: Select a member ID, find all logins associated with the member, put them into a DataFrame and give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example time series using the below customer code\n",
    "ex_cust_code = \"8000938.80\"\n",
    "\n",
    "ex_data = login_data[login_data[\"CUST_CODE\"] == ex_cust_code]\n",
    "\n",
    "ex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logins that have a deleted customer code\n",
    "no_cust_code = login_data[login_data[\"CUST_CODE\"] == \"~del_cust\"]\n",
    "\n",
    "#try:\n",
    "#    login_data_sanitized = pd.read_csv(\"login_data_sanitized.csv\")\n",
    "#except Exception as e:\n",
    "login_data_sanitized = login_data.drop(no_cust_code.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove logins with NaN customer codes\n",
    "nan_cust_code = login_data_sanitized[login_data_sanitized[\"CUST_CODE\"].isnull()]\n",
    "\n",
    "login_data_sanitized.drop(nan_cust_code.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boolean mask telling which customers from the customer base have actually had a login\n",
    "user_has_no_login = ~sanitized_customer_base.index.isin(login_data_sanitized[\"CUST_CODE\"])\n",
    "\n",
    "users_without_login = sanitized_customer_base[user_has_no_login]\n",
    "print(\"{0} customers out of {1} total customers have not registered a login.\".format(len(users_without_login), len(sanitized_customer_base.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that about 15000 of our total (processed) customer base of 158231 do not have a single login. As the analysis of their login data is nonsensical, we drop these users as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_customer_base.drop(users_without_login.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_customer_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert login dates into timestamps for easier time series analysis\n",
    "login_date_cols = [\"DATE_SAVED\", \"DATE_LAST_MODIFIED\"]\n",
    "\n",
    "for col in login_date_cols:\n",
    "    #makes the parser convert wrong timestamps into NaT (not a timestamp)\n",
    "    login_data_sanitized[col] = pd.to_datetime(login_data_sanitized[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_data_sanitized[\"DATE\"] = [ d.date() for d in login_data_sanitized[\"DATE_SAVED\"]]\n",
    "login_data_sanitized[\"MONTH\"] = [ d.month for d in login_data_sanitized[\"DATE_SAVED\"]]\n",
    "login_data_sanitized[\"WEEK\"] = [ d.week for d in login_data_sanitized[\"DATE_SAVED\"]]\n",
    "login_data_sanitized[\"DAYOFWEEK\"] = [ d.dayofweek for d in login_data_sanitized[\"DATE_SAVED\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_data_sanitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_logins = login_data_sanitized[~login_data_sanitized.CUST_CODE.isin(sanitized_customer_base.index)]\n",
    "dead_logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_data_sanitized.drop(dead_logins.index, inplace=True)\n",
    "login_data_sanitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_customer_base[\"NUM_VISITS\"] = login_data_sanitized.CUST_CODE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stat_utils import calculate_unittest_pvals\n",
    "#construct result dataframe\n",
    "stat_types = [\"kpss_c\", \"kpss_ct\", \"adf_c\", \"adf_ct\"]\n",
    "\n",
    "results = pd.DataFrame(np.nan, columns=stat_types, index=sanitized_customer_base.index)\n",
    "\n",
    "#Possible TODO: Make an option to select active range instead of contract duration\n",
    "last_log_date = login_data_sanitized[\"DATE_SAVED\"].max()   \n",
    "\n",
    "kwds = {\"customer_base\": sanitized_customer_base, \"last_log_date\": last_log_date, \"stat_names\": stat_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "from tqdm.auto import tqdm \n",
    "#from pandarallel import pandarallel\n",
    "#initialize parallel pandas\n",
    "#pandarallel.initialize(nb_workers=4, progress_bar=False, verbose=verbose)\n",
    "tqdm.pandas()\n",
    "results = login_data_sanitized.groupby(\"CUST_CODE\").progress_apply(calculate_unittest_pvals, **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_user_base = pd.concat([sanitized_customer_base, results], axis=1)\n",
    "augmented_user_base.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from utils.time_utils import construct_binary_visit_series\n",
    "\n",
    "last_log_date = login_data_sanitized[\"DATE_SAVED\"].max()\n",
    "sample_cust = sanitized_customer_base.sample(n=1).index[0]\n",
    "\n",
    "#total number of calculated statistics\n",
    "total_stats = 4\n",
    "\n",
    "stats = np.full(total_stats, np.nan)\n",
    "\n",
    "data = login_data_sanitized[login_data_sanitized[\"CUST_CODE\"] == sample_cust]\n",
    " \n",
    "customer = sanitized_customer_base.loc[sample_cust]\n",
    "\n",
    "#after this, visit_series contains integers \n",
    "binary_visit_ts = construct_binary_visit_series(customer, data, last_log_date).astype(float)\n",
    "\n",
    "#save p-value of kpss test for later significance level setting\n",
    "stats[0] = kpss(binary_visit_ts, regression=\"c\", nlags=\"auto\")[1]\n",
    "stats[1] = kpss(binary_visit_ts, regression=\"ct\", nlags=\"auto\")[1]\n",
    "\n",
    "#save p-value of ADF test \n",
    "stats[2] = adfuller(binary_visit_ts, regression=\"c\")[1]\n",
    "stats[3] = adfuller(binary_visit_ts, regression=\"ct\")[1]\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login_data_sanitized.groupby([\"DATE\"]).count()\n",
    "login_data_sanitized.to_csv(\"../DATA_GYM/sanitized_login_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the sanitized customer base \n",
    "sanitized_customer_base.to_csv(\"../DATA_GYM/sanitized_customer_base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_counts = login_data_sanitized[\"CUST_CODE\"].value_counts()\n",
    "login_counts.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_counts[login_counts <= 50].hist(bins=50)\n",
    "plt.savefig(\"../results/visit_modelling/login_hist.jpg\", my_dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets the login date as the index of login_data_sanitized. \n",
    "#Running twice will result in a KeyError exception\n",
    "login_data_sanitized = login_data_sanitized.set_index(\"DATE_SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_logins.index = pd.to_datetime(daily_logins.index)\n",
    "weekly_mean_logins = daily_logins.resample('W').sum()\n",
    "\n",
    "weekly_mean_logins[\"RES_ID\"].plot(linewidth=0.5)\n",
    "print(weekly_mean_logins[\"RES_ID\"])\n",
    "print(daily_logins[\"RES_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_logins[\"RES_ID\"].diff(periods=1).plot(linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_logins[\"RES_ID\"].diff(periods=2).plot(linewidth=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
